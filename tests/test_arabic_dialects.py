"""
Arabic dialect and mixed content testing
Comprehensive testing for different Arabic dialects and edge cases
"""

import pytest
import asyncio
from typing import Dict, List
from utils.arabic_processor import ArabicTextProcessor, process_arabic_text, extract_sentiment
from utils.openai_client import analyze_arabic_feedback

class TestArabicDialects:
    """Test Arabic dialect processing"""
    
    def setup_method(self):
        """Setup test data for Arabic dialects"""
        self.processor = ArabicTextProcessor()
        
        # Dialect test data
        self.dialect_samples = {
            "gulf": [
                "Ø§Ù„Ø®Ø¯Ù…Ø© Ø²ÙŠÙ†Ø© ÙˆÙ…Ø´ÙƒÙˆØ±ÙŠÙ† Ø¹Ù„Ù‰ Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ø§Ù„Ø·ÙŠØ¨",  # Service is good, thanks for good treatment
                "Ù…Ø§ Ø´Ø§Ø¡ Ø§Ù„Ù„Ù‡ Ø§Ù„Ù…ÙˆÙ‚Ø¹ ÙŠÙ‡Ø¨Ù„ ÙˆØ³Ù‡Ù„ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…",  # Excellent website, easy to use
                "ÙˆØ§Ù„Ù„Ù‡ Ø§Ù„Ø¹Ø¸ÙŠÙ… Ø§Ù„Ù…Ù†ØªØ¬ Ø®Ø±Ø§ÙÙŠ ÙˆÙŠÙ†ØµØ­ ÙÙŠÙ‡ Ø¨Ù‚ÙˆØ©",  # Excellent product, highly recommended
                "ÙŠØ¹Ø·ÙŠÙƒÙ… Ø§Ù„Ø¹Ø§ÙÙŠØ© Ø¹Ù„Ù‰ Ø§Ù„Ø¬Ù‡Ø¯ Ø§Ù„Ø±Ø§Ø¦Ø¹",  # Great work, well done
                "Ø§Ù„Ù„Ù‡ ÙŠØ¹Ø·ÙŠÙƒÙ… Ø§Ù„Ø¹Ø§ÙÙŠØ© ÙˆØ§Ù„ØªÙˆÙÙŠÙ‚ Ø¥Ù† Ø´Ø§Ø¡ Ø§Ù„Ù„Ù‡"  # Good luck and success
            ],
            "egyptian": [
                "Ø§Ù„Ø­Ù…Ø¯ Ù„Ù„Ù‡ Ø§Ù„Ø®Ø¯Ù…Ø© ÙƒÙˆÙŠØ³Ø© Ø¬Ø¯Ø§Ù‹ ÙˆÙ…ÙÙŠØ´ Ø£ÙŠ Ù…Ø´Ø§ÙƒÙ„",  # Service is very good, no problems
                "Ø¨Ø¬Ø¯ Ø§Ù„Ù…Ù†ØªØ¬ Ø¯Ù‡ Ø¹Ø¬Ø¨Ù†ÙŠ Ø£ÙˆÙŠ ÙˆØ§Ù„Ù†Ø§Ø³ ÙƒÙ„Ù‡Ø§ Ø¨ØªÙ…Ø¯Ø­Ù‡",  # Really liked this product, everyone praises it
                "Ù…Ø§ Ø´Ø§Ø¡ Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙƒÙ… Ø´ØºÙ„ Ù…Ø­ØªØ±Ù… ÙˆÙ†Ø§Ø³ Ù…Ø­ØªØ±Ù…Ø©",  # Professional work and people
                "Ø¨ØµØ±Ø§Ø­Ø© Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø¯Ù‡ Ø­Ù„Ùˆ ÙˆØ³Ù‡Ù„ ÙÙŠ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…",  # Honestly, this app is nice and easy to use
                "Ø±Ø¨Ù†Ø§ ÙŠØ¨Ø§Ø±Ùƒ ÙÙŠÙƒÙ… ÙˆØ§Ù„Ù„Ù‡ Ø´ØºÙ„ Ù…Ù…ØªØ§Ø²"  # God bless you, excellent work
            ],
            "levantine": [
                "ÙˆØ§Ù„Ù„Ù‡ Ø§Ù„Ø®Ø¯Ù…Ø© ÙƒØªÙŠØ± Ù…Ù†ÙŠØ­Ø© ÙˆÙ…Ø´Ø§Ù† Ù‡ÙŠÙƒ Ø¨Ù†ØµØ­ ÙÙŠÙ‡Ø§",  # Service is very good, that's why we recommend it
                "Ø¨ØµØ±Ø§Ø­Ø© Ø§Ù„Ù…Ù†ØªØ¬ Ø¹Ø¬Ø¨Ù†ÙŠ ÙƒØªÙŠØ± ÙˆÙ‡Ø§Ø¯ Ø´ÙŠ Ø¨ÙŠÙØ±Ø­Ù†ÙŠ",  # Honestly, I really liked the product
                "ÙŠØ¹Ø·ÙŠÙƒÙ… Ø§Ù„Ø¹Ø§ÙÙŠØ© Ø¹Ù„Ù‰ Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ø§Ù„Ø±Ø§Ù‚ÙŠ ÙˆØ§Ù„Ù…Ø­ØªØ±Ù…",  # Thank you for the excellent service
                "Ù…Ø§ Ø´Ø§Ø¡ Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙƒÙ… Ø´ØºÙ„ Ù†Ø¸ÙŠÙ ÙˆÙ…Ø±ØªØ¨",  # Clean and organized work
                "Ø§Ù„Ù„Ù‡ ÙŠØ¹Ø·ÙŠÙƒÙ… Ø§Ù„Ù‚ÙˆØ© ÙˆØ§Ù„ØµØ­Ø© ÙˆØ§Ù„Ø¹Ø§ÙÙŠØ©"  # God give you strength and health
            ],
            "moroccan": [
                "Ø§Ù„Ø®Ø¯Ù…Ø© Ø²ÙˆÙŠÙ†Ø© Ø¨Ø²Ø§Ù ÙˆØ§Ù„Ù†Ø§Ø³ Ù…Ø²ÙŠØ§Ù†ÙŠÙ† Ù…Ø¹Ø§Ù†Ø§",  # Service is very good, people are nice
                "Ø¨ØµØ­ Ù‡Ø§Ø¯ Ø§Ù„Ù…Ù†ØªØ¬ Ø¹Ø¬Ø¨Ù†ÙŠ ÙƒØªØ± ÙˆÙƒØ§ÙŠÙ† Ù„ÙŠ ÙÙŠÙ‡",  # This product really impressed me
                "Ù…Ø§Ø´ÙŠ Ù…Ø´ÙƒÙ„ Ø§Ù„Ø®Ø¯Ù…Ø© ÙÙˆÙ‚ Ø§Ù„Ù…Ù…ØªØ§Ø² ÙˆØ§Ù„Ù†Ø§Ø³ Ù…Ø±Ø­Ø¨ÙŠÙ†",  # No problem, service is excellent
                "Ø§Ù„Ù„Ù‡ ÙŠØ¹Ø·ÙŠÙƒÙ… Ø§Ù„ØµØ­Ø© Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¬Ù‡ÙˆØ¯ Ø§Ù„ÙƒØ¨ÙŠØ±",  # God give you health for the great effort
                "ÙˆØ§Ø®Ø§ Ù‡ÙƒØ§Ùƒ Ø§Ù„Ø´ÙŠ Ø²ÙˆÙŠÙ† ÙˆØ§Ù„Ø®Ø¯Ù…Ø© Ù…Ù…ØªØ§Ø²Ø©"  # Everything is good and service is excellent
            ]
        }
        
        # Mixed content samples
        self.mixed_content_samples = [
            "Ø§Ù„Ø®Ø¯Ù…Ø© excellent ÙˆØ§Ù„ÙØ±ÙŠÙ‚ professional Ø¬Ø¯Ø§Ù‹",
            "Thank you ÙƒØªÙŠØ± Ø¹Ù„Ù‰ Ø§Ù„Ø®Ø¯Ù…Ø© Ø§Ù„Ø±Ø§Ø¦Ø¹Ø© ÙˆØ§Ù„Ø¯Ø¹Ù… Ø§Ù„Ù…Ø³ØªÙ…Ø±",
            "Ø§Ù„Ù…Ù†ØªØ¬ amazing Ø¨Ø³ Ø§Ù„Ù€ delivery ÙƒØ§Ù† Ù…ØªØ£Ø®Ø± Ø´ÙˆÙŠ",
            "Overall Ø§Ù„Ø®Ø¯Ù…Ø© Ù…Ù…ØªØ§Ø²Ø© but need improvement ÙÙŠ Ø§Ù„Ù€ response time",
            "Ù…ÙˆÙ‚Ø¹ÙƒÙ… website Ø¬Ù…ÙŠÙ„ ÙˆØ§Ù„Ù€ UI user-friendly",
            "Ø§Ù„Ù€ customer service Ø±Ø§Ø¦Ø¹ ÙˆØ§Ù„Ù€ staff helpful ÙƒØªÙŠØ±"
        ]
        
        # Long text samples
        self.long_text_samples = [
            "Ø¨Ø³Ù… Ø§Ù„Ù„Ù‡ Ø§Ù„Ø±Ø­Ù…Ù† Ø§Ù„Ø±Ø­ÙŠÙ…ØŒ Ø£ÙˆØ¯ Ø£Ù† Ø£Ø¹Ø¨Ø± Ø¹Ù† Ø§Ù…ØªÙ†Ø§Ù†ÙŠ Ø§Ù„Ø¹Ù…ÙŠÙ‚ ÙˆØ´ÙƒØ±ÙŠ Ø§Ù„Ø¬Ø²ÙŠÙ„ Ù„ÙƒÙ… Ø¹Ù„Ù‰ Ø§Ù„Ø®Ø¯Ù…Ø© Ø§Ù„Ø§Ø³ØªØ«Ù†Ø§Ø¦ÙŠØ© Ø§Ù„ØªÙŠ ØªÙ‚Ø¯Ù…ÙˆÙ†Ù‡Ø§. " * 10,
            "Ù„Ù‚Ø¯ ÙƒØ§Ù†Øª ØªØ¬Ø±Ø¨ØªÙŠ Ù…Ø¹ Ø´Ø±ÙƒØªÙƒÙ… Ø§Ù„Ù…Ø­ØªØ±Ù…Ø© ØªØ¬Ø±Ø¨Ø© Ø±Ø§Ø¦Ø¹Ø© ÙˆÙ…Ù…ÙŠØ²Ø© Ù…Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†ÙˆØ§Ø­ÙŠØŒ Ø­ÙŠØ« Ù„Ù…Ø³Øª Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù… Ø§Ù„ÙƒØ¨ÙŠØ± ÙˆØ§Ù„Ø¹Ù†Ø§ÙŠØ© Ø§Ù„ÙØ§Ø¦Ù‚Ø© " * 15,
            "Ø¥Ù† Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø¬ÙˆØ¯Ø© Ø§Ù„Ø¹Ø§Ù„ÙŠ ÙˆØ§Ù„Ø®Ø¯Ù…Ø© Ø§Ù„Ù…ØªÙ…ÙŠØ²Ø© Ø§Ù„ØªÙŠ ØªÙ‚Ø¯Ù…ÙˆÙ†Ù‡Ø§ ØªØ¹ÙƒØ³ Ù…Ø¯Ù‰ Ø­Ø±ØµÙƒÙ… Ø¹Ù„Ù‰ Ø¥Ø±Ø¶Ø§Ø¡ Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡ ÙˆØªØ­Ù‚ÙŠÙ‚ ØªÙˆÙ‚Ø¹Ø§ØªÙ‡Ù… " * 12
        ]
        
        # Special characters and diacritics
        self.diacritics_samples = [
            "Ø§Ù„Ù’Ø­ÙŽÙ…Ù’Ø¯Ù Ù„ÙÙ„ÙŽÙ‘Ù‡Ù Ø±ÙŽØ¨ÙÙ‘ Ø§Ù„Ù’Ø¹ÙŽØ§Ù„ÙŽÙ…ÙÙŠÙ†ÙŽ",  # With full diacritics
            "Ù…ÙŽØ±Ù’Ø­ÙŽØ¨Ù‹Ø§ Ø¨ÙÙƒÙÙ…Ù’ ÙÙÙŠ Ù…ÙŽÙˆÙ’Ù‚ÙØ¹ÙÙ†ÙŽØ§ Ø§Ù„Ù’Ù…ÙØªÙŽÙ…ÙŽÙŠÙÙ‘Ø²Ù",  # Welcome with diacritics
            "Ø´ÙÙƒÙ’Ø±Ù‹Ø§ Ù„ÙŽÙƒÙÙ…Ù’ Ø¹ÙŽÙ„ÙŽÙ‰ Ø§Ù„Ù’Ø®ÙØ¯Ù’Ù…ÙŽØ©Ù Ø§Ù„Ø±ÙŽÙ‘Ø§Ø¦ÙØ¹ÙŽØ©Ù",  # Thanks with diacritics
            "Ù†ÙŽØ­Ù’Ù†Ù Ù†ÙÙ‚ÙŽØ¯ÙÙ‘Ø±Ù Ø¬ÙÙ‡ÙÙˆØ¯ÙŽÙƒÙÙ…Ù Ø§Ù„Ù’Ù…ÙØªÙŽÙˆÙŽØ§ØµÙÙ„ÙŽØ©ÙŽ",  # We appreciate with diacritics
            "Ø¨ÙŽØ§Ø±ÙŽÙƒÙŽ Ø§Ù„Ù„ÙŽÙ‘Ù‡Ù ÙÙÙŠÙƒÙÙ…Ù’ ÙˆÙŽÙÙÙŠ Ø¹ÙŽÙ…ÙŽÙ„ÙÙƒÙÙ…Ù Ø§Ù„Ù’Ù…ÙØ¨ÙŽØ§Ø±ÙŽÙƒÙ"  # Blessed work with diacritics
        ]
    
    def test_gulf_dialect_processing(self):
        """Test Gulf Arabic dialect processing"""
        for text in self.dialect_samples["gulf"]:
            # Test Arabic detection
            assert self.processor.is_arabic_text(text) == True
            
            # Test normalization
            normalized = self.processor.normalize_arabic(text)
            assert isinstance(normalized, str)
            assert len(normalized) > 0
            
            # Test sentiment extraction
            sentiment = extract_sentiment(text)
            assert isinstance(sentiment, dict)
            # Check for either 'score' or 'sentiment' key (backward compatibility)
            score = sentiment.get("score", sentiment.get("sentiment", 0))
            confidence = sentiment.get("confidence", 0)
            assert score is not None
            assert confidence is not None
            # Gulf expressions should generally be positive
            assert score >= 0
    
    def test_egyptian_dialect_processing(self):
        """Test Egyptian Arabic dialect processing"""
        for text in self.dialect_samples["egyptian"]:
            # Test processing
            processed = process_arabic_text(text)
            assert isinstance(processed, str)
            assert len(processed) > 0
            
            # Test keyword extraction
            keywords = self.processor.extract_keywords(text)
            assert isinstance(keywords, list)
            
            # Test emotion detection
            emotions = self.processor.detect_emotion_words(text)
            assert isinstance(emotions, dict)
            assert all(key in emotions for key in ['positive', 'negative', 'neutral'])
    
    def test_levantine_dialect_processing(self):
        """Test Levantine Arabic dialect processing"""
        for text in self.dialect_samples["levantine"]:
            # Test reshaping for display
            reshaped = self.processor.reshape_for_display(text)
            assert isinstance(reshaped, str)
            
            # Test sentiment analysis
            sentiment = extract_sentiment(text)
            # Levantine expressions should generally be positive
            assert sentiment["score"] >= -0.5  # Allow for some variation
    
    def test_moroccan_dialect_processing(self):
        """Test Moroccan Arabic dialect processing"""
        for text in self.dialect_samples["moroccan"]:
            # Test basic processing
            is_arabic = self.processor.is_arabic_text(text)
            assert is_arabic == True
            
            # Test normalization (may need special handling for Moroccan)
            normalized = self.processor.normalize_arabic(text)
            assert isinstance(normalized, str)
    
    def test_mixed_content_processing(self):
        """Test mixed Arabic-English content processing"""
        for text in self.mixed_content_samples:
            # Should still be detected as Arabic (dominant language)
            is_arabic = self.processor.is_arabic_text(text)
            # Mixed content might not always be detected as Arabic
            
            # Should process without errors
            processed = process_arabic_text(text)
            assert isinstance(processed, str)
            assert len(processed) > 0
            
            # Sentiment analysis should work
            sentiment = extract_sentiment(text)
            assert isinstance(sentiment, dict)
            assert -1 <= sentiment["score"] <= 1
    
    def test_long_text_processing(self):
        """Test processing of long Arabic texts"""
        for text in self.long_text_samples:
            assert len(text) > 500  # Ensure it's actually long
            
            # Test performance with long text
            import time
            start_time = time.time()
            
            processed = process_arabic_text(text)
            
            end_time = time.time()
            processing_time = end_time - start_time
            
            # Should complete within reasonable time
            assert processing_time < 5.0  # 5 seconds max
            assert isinstance(processed, str)
            assert len(processed) > 0
    
    def test_diacritics_processing(self):
        """Test processing of Arabic text with diacritics"""
        for text in self.diacritics_samples:
            # Should be detected as Arabic
            assert self.processor.is_arabic_text(text) == True
            
            # Normalization should handle diacritics
            normalized = self.processor.normalize_arabic(text)
            assert isinstance(normalized, str)
            
            # Normalized text should be shorter (diacritics removed)
            assert len(normalized) <= len(text)
            
            # Should not contain diacritics after normalization
            diacritics = 'Ù‹ÙŒÙÙŽÙÙÙ‘Ù’'
            for diacritic in diacritics:
                assert diacritic not in normalized
    
    def test_dialect_sentiment_consistency(self):
        """Test sentiment analysis consistency across dialects"""
        # Positive samples from each dialect
        positive_samples = [
            self.dialect_samples["gulf"][0],  # Service is good
            self.dialect_samples["egyptian"][0],  # Service is very good
            self.dialect_samples["levantine"][0],  # Service is very good
            self.dialect_samples["moroccan"][0]  # Service is very good
        ]
        
        sentiments = []
        for text in positive_samples:
            sentiment = extract_sentiment(text)
            score = sentiment.get("score", sentiment.get("sentiment", 0))
            sentiments.append(score)
        
        # All should be positive
        for score in sentiments:
            assert score > 0, f"Expected positive sentiment, got {score}"
        
        # Variance should not be too high (dialect consistency)
        import statistics
        if len(sentiments) > 1:
            std_dev = statistics.stdev(sentiments)
            assert std_dev < 0.5, f"High variance in sentiment scores: {std_dev}"
    
    def test_special_characters_handling(self):
        """Test handling of special characters and symbols"""
        special_texts = [
            "Ø§Ù„Ø®Ø¯Ù…Ø© Ù…Ù…ØªØ§Ø²Ø© ðŸ˜ŠðŸ‘ ÙˆØ§Ù„Ø¯Ø¹Ù… Ø±Ø§Ø¦Ø¹! ðŸ’¯",  # With emojis
            "Ø§Ù„ØªÙ‚ÙŠÙŠÙ…: â­â­â­â­â­ Ø®Ù…Ø³ Ù†Ø¬ÙˆÙ…",  # With star ratings
            "Ø±Ù‚Ù… Ø§Ù„Ù‡Ø§ØªÙ: Ù Ù¡Ù¢Ù£Ù¤Ù¥Ù¦Ù§Ù¨Ù© Ù„Ù„Ø§Ø³ØªÙØ³Ø§Ø±",  # With Arabic numerals
            "Ø§Ù„Ø¨Ø±ÙŠØ¯ Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ: test@example.com",  # With email
            "Ø§Ù„Ù…ÙˆÙ‚Ø¹: https://example.com/arabic",  # With URL
            "Ø§Ù„Ø³Ø¹Ø±: Ù¡Ù Ù  Ø±ÙŠØ§Ù„ Ø³Ø¹ÙˆØ¯ÙŠ (100 SAR)"  # Mixed numerals
        ]
        
        for text in special_texts:
            # Should process without errors
            try:
                processed = process_arabic_text(text)
                sentiment = extract_sentiment(text)
                
                assert isinstance(processed, str)
                assert isinstance(sentiment, dict)
                
            except Exception as e:
                pytest.fail(f"Special character processing failed for '{text}': {e}")
    
    def test_empty_and_edge_cases(self):
        """Test empty strings and edge cases"""
        edge_cases = [
            "",  # Empty string
            "   ",  # Whitespace only
            "123456",  # Numbers only
            "!@#$%^&*()",  # Symbols only
            "a",  # Single character
            "Ø£",  # Single Arabic character
            "Ø§Ù„Ù€",  # Arabic definite article only
            "Ø§Ø®ØªØ¨Ø§Ø±" * 1000  # Very long repetitive text
        ]
        
        for text in edge_cases:
            try:
                # Should handle gracefully without crashing
                is_arabic = self.processor.is_arabic_text(text)
                assert isinstance(is_arabic, bool)
                
                if text.strip():  # Non-empty
                    processed = process_arabic_text(text)
                    assert isinstance(processed, str)
                    
            except Exception as e:
                # Should not crash, but may return default values
                assert isinstance(e, (ValueError, TypeError))

class TestPerformanceWithDialects:
    """Test performance with various dialect content"""
    
    def test_batch_dialect_processing(self):
        """Test processing multiple dialects in batch"""
        all_dialect_texts = []
        
        # Collect all dialect samples
        dialect_data = TestArabicDialects().dialect_samples
        for dialect_texts in dialect_data.values():
            all_dialect_texts.extend(dialect_texts)
        
        # Test batch processing performance
        import time
        start_time = time.time()
        
        results = []
        for text in all_dialect_texts:
            sentiment = extract_sentiment(text)
            results.append(sentiment)
        
        end_time = time.time()
        total_time = end_time - start_time
        
        # Should complete within reasonable time
        texts_per_second = len(all_dialect_texts) / total_time
        assert texts_per_second > 5, f"Processing too slow: {texts_per_second} texts/sec"
        
        # All results should be valid
        for result in results:
            assert isinstance(result, dict)
            assert "score" in result
            assert "confidence" in result
    
    def test_concurrent_dialect_processing(self):
        """Test concurrent processing of different dialects"""
        import concurrent.futures
        import threading
        
        dialect_data = TestArabicDialects().dialect_samples
        
        def process_dialect_batch(dialect_name, texts):
            """Process a batch of dialect texts"""
            results = []
            for text in texts:
                try:
                    sentiment = extract_sentiment(text)
                    results.append((dialect_name, sentiment))
                except Exception as e:
                    results.append((dialect_name, {"error": str(e)}))
            return results
        
        # Process dialects concurrently
        with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
            futures = []
            for dialect_name, texts in dialect_data.items():
                future = executor.submit(process_dialect_batch, dialect_name, texts)
                futures.append(future)
            
            # Collect results
            all_results = []
            for future in concurrent.futures.as_completed(futures):
                results = future.result()
                all_results.extend(results)
        
        # Verify all dialects were processed
        processed_dialects = set(result[0] for result in all_results)
        expected_dialects = set(dialect_data.keys())
        assert processed_dialects == expected_dialects
        
        # Verify no errors occurred
        error_results = [r for r in all_results if "error" in r[1]]
        assert len(error_results) == 0, f"Errors in concurrent processing: {error_results}"

class TestArabicContentValidation:
    """Test validation of Arabic content quality and authenticity"""
    
    def test_authentic_vs_translated_content(self):
        """Test detection of authentic vs translated Arabic content"""
        # Authentic Arabic expressions
        authentic_samples = [
            "Ø¨Ø§Ø±Ùƒ Ø§Ù„Ù„Ù‡ ÙÙŠÙƒÙ… Ø¹Ù„Ù‰ Ù‡Ø°Ø§ Ø§Ù„Ø¹Ù…Ù„ Ø§Ù„Ù…ØªÙ…ÙŠØ²",
            "Ø¬Ø²Ø§ÙƒÙ… Ø§Ù„Ù„Ù‡ Ø®ÙŠØ±Ø§Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø®Ø¯Ù…Ø© Ø§Ù„Ø±Ø§Ø¦Ø¹Ø©",
            "Ù…Ø§ Ø´Ø§Ø¡ Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙƒÙ… ÙˆØ§Ù„Ù„Ù‡ ÙŠÙˆÙÙ‚ÙƒÙ…",
            "Ø§Ù„Ù„Ù‡ ÙŠØ¹Ø·ÙŠÙƒÙ… Ø§Ù„Ø¹Ø§ÙÙŠØ© ÙˆØ§Ù„Ù‚ÙˆØ©"
        ]
        
        # Potentially translated content (more literal)
        translated_samples = [
            "Ø´ÙƒØ±Ø§Ù‹ Ù„Ùƒ Ø¹Ù„Ù‰ Ø§Ù„Ø®Ø¯Ù…Ø© Ø§Ù„Ø¬ÙŠØ¯Ø© Ø¬Ø¯Ø§Ù‹",
            "Ø£Ù†Ø§ Ø±Ø§Ø¶ÙŠ Ø¬Ø¯Ø§Ù‹ Ø¹Ù† Ù‡Ø°Ø§ Ø§Ù„Ù…Ù†ØªØ¬",
            "Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø³Ù‡Ù„ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¬Ø¯Ø§Ù‹",
            "Ø£Ù†ØµØ­ Ø¨Ù‡Ø°Ù‡ Ø§Ù„Ø´Ø±ÙƒØ© Ø¨Ù‚ÙˆØ©"
        ]
        
        processor = ArabicTextProcessor()
        
        # Both should be detected as Arabic
        for text in authentic_samples + translated_samples:
            assert processor.is_arabic_text(text) == True
        
        # Authentic content might have more cultural markers
        authentic_keywords = []
        translated_keywords = []
        
        for text in authentic_samples:
            keywords = processor.extract_keywords(text)
            authentic_keywords.extend(keywords)
        
        for text in translated_samples:
            keywords = processor.extract_keywords(text)
            translated_keywords.extend(keywords)
        
        # Check for cultural/religious expressions in authentic content
        cultural_markers = ['Ø§Ù„Ù„Ù‡', 'Ø¨Ø§Ø±Ùƒ', 'Ø¬Ø²Ø§ÙƒÙ…', 'Ù…Ø§ Ø´Ø§Ø¡']
        authentic_cultural_count = sum(1 for marker in cultural_markers 
                                     if any(marker in keyword for keyword in authentic_keywords))
        
        translated_cultural_count = sum(1 for marker in cultural_markers 
                                      if any(marker in keyword for keyword in translated_keywords))
        
        # Authentic content should have more cultural markers
        assert authentic_cultural_count >= translated_cultural_count
    
    def test_content_quality_metrics(self):
        """Test quality metrics for Arabic content"""
        high_quality_samples = [
            "Ø£ØªÙ‚Ø¯Ù… Ø¨Ø¬Ø²ÙŠÙ„ Ø§Ù„Ø´ÙƒØ± ÙˆØ§Ù„Ø§Ù…ØªÙ†Ø§Ù† Ù„ÙØ±ÙŠÙ‚ Ø§Ù„Ø¹Ù…Ù„ Ø§Ù„Ù…Ø­ØªØ±Ù… Ø¹Ù„Ù‰ Ø§Ù„Ø®Ø¯Ù…Ø© Ø§Ù„Ø§Ø³ØªØ«Ù†Ø§Ø¦ÙŠØ©",
            "Ù„Ù‚Ø¯ ÙØ§Ù‚Øª Ø§Ù„Ø®Ø¯Ù…Ø© ØªÙˆÙ‚Ø¹Ø§ØªÙŠ Ø¨Ø´ÙƒÙ„ ÙƒØ¨ÙŠØ± ÙˆØ£Ù†ØµØ­ Ø¨Ù‡Ø§ Ø¯ÙˆÙ† ØªØ±Ø¯Ø¯",
            "Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù… Ø¨ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ø¹Ù…ÙŠÙ„ ÙˆØ§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø§Ù„Ø³Ø±ÙŠØ¹Ø© ÙŠØ¯Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø­ØªØ±Ø§ÙÙŠØ© Ø§Ù„Ø¹Ø§Ù„ÙŠØ©"
        ]
        
        low_quality_samples = [
            "Ø²ÙŠÙ†",  # Too short
            "Ø§Ù„Ù…Ù†ØªØ¬ Ø²ÙŠÙ† Ø¨Ø³ ØºØ§Ù„ÙŠ Ø´ÙˆÙŠ",  # Informal/incomplete
            "Ø§ÙˆÙƒ",  # Very short/informal
        ]
        
        processor = ArabicTextProcessor()
        
        for text in high_quality_samples:
            # High quality should have more keywords
            keywords = processor.extract_keywords(text)
            assert len(keywords) >= 3
            
            # Should have reasonable length
            assert len(text.split()) >= 5
        
        for text in low_quality_samples:
            # Low quality might have fewer keywords
            keywords = processor.extract_keywords(text)
            # Should still process without errors
            assert isinstance(keywords, list)